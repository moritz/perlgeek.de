[%setvar section article%]
[%setvar title Character Encodings in Perl %]
[%menu main article unicode %]
[%setvar de /de/artikel/charsets-unicode %]
<!-- INDEX BEGIN -->

    <h1>[%readvar title%]</h1>

<p>
    This article describes the different character encodings, how they may
    lead to problems, and how they can be handled in Perl programs.
</p>

<h2><a name="intro"></a>Introduction</h2>

<p>
    It happens far too often: a program works fine with latin characters, but
    it produces weird, unreadable characters as soon as it has to process 
    other characters like Chinese or Japanese characters or modified latin 
    characters like the German Umlauts Ä, Ö etc. or the Scandinavian
    characters å and Ø.
</p>

    <h2 id="ascii">ASCII</h2>
<p>
    To understand the root of the problem you have to understand how "normal"
    Latin characters and other characters (the ones that cause problems) are 
    stored.
</p>

<p>
    It all began in the year 1963 with ASCII, the "American Standard for
    Information Interchange". It maps 128 characters to the number from 0 to
    127, which can be encoded with 7 bits.
</p>

<p>
    Since a byte contains 8 bits, the first, "most significant" bit in ASCII
    characters is always zero.
</p>

<p>
    The standard defines the Latin letters <code>a</code> to <code>z</code> in
    both upper and lower case, the Arabic digits <code>0</code> to
    <code>9</code>, whitespace like "blank" and "carriage return", a few
    control characters and a few special signs like <code>%</code>,
    <code>$</code> and so on.
</p>

<p>
    Characters that aren't essential in the day to day life of an American
    citizen are not defined in ASCII, like Cyrillic letters, "decorated" Latin
    characters, Greek characters and so on.
</p>

    <h2 id="other_charsets">Other Character Encodings</h2>

<p>
    When people started to use computers in other countries, other
    characters needed to be encoded. In the European countries ASCII was
    reused, and the 128 unused numbers per byte were used for the locally
    needed characters.
</p>

<p>
    In Western Europe the character encoding was called "Latin 1", and later
    standardized as ISO-8859-1. Latin 2 was used in central Europe and so on.
</p>

<p>
    In each of the Latin-* charsets the first 128 characters are identical to
    ASCII, so they can be viewed as ASCII extensions. The second 128 byte
    values are each mapped to characters needed in the regions where these
    character sets were used.
</p>

[% comment more stuff here, and check it
<p>
    Other commonly used character encodings are the IBM code pages like <a
    href="http://en.wikipedia.org/wiki/Code_page_850">Code Page 850</a>
    (similar to Latin-1), some of which were later superseded by Microsoft
    Windows specific encodings like <a
    href="http://en.wikipedia.org/wiki/Windows-1252">Windows-1252</a>.
%]


[% comment TODO: check the Shift-JIS part %]
<p>
    In other parts of world other character encodings were developed, like
    EUC-CN in China and Shift-JIS in Japan.
</p>

<p>
    These local charsets are very limited. When the Euro was introduced in
    2001, many European countries had a currencies whose symbols couldn't be
    expressed in the traditional character encodings.
</p>

<h2><a name="unicode"></a>Unicode</h2>

<p>
    The charsets mentioned so far can encode only a small part of all possible
    characters, which makes it nearly impossible to create documents that
    contain letters from different scripts.
</p>

<p>
    In an attempt to unify all scripts into a single writing system, the
    Unicode consortium was created, and it started to collect all known
    characters, and assign a unique number to each, called a "codepoint".
</p>

<p>
    The codepoint is usually written as a four or six digit hex number, like
    <code>U+0041</code>. The corresponding name is <code>LATIN SMALL LETTER A</code>.
</p>

<p>
    Apart from letters and other "base characters", there are also accents
    and decorations like <code>ACCENT, COMBINING ACUTE</code>, which can be
    added to a base character.
</p>
    
<p>
    If a base char is followed by one or more of these marking characters, 
    this compound forms a logical character called "grapheme".
</p>

<p>
    Note that many pre-composed graphemes exist for characters that are
    defined in other character sets, and these pre-composed are typically
    better supported by current software than the equivalent written as base
    character and combining mark.
</p>
    

<h3><a name="unicode_transformation_formats"></a>Unicode Transformation Formats</h3>

<p>
    The concept of Unicode codepoints and graphemes is completely independent
    of the encoding.
</p>

<p>
    There are different ways to encode these codepoints, and these mappings
    from codepoints to bytes are called "Unicode Transformation Formats".
    The most well known is UTF-8, which is a byte based format that uses all
    possible byte values from 0 to 255. In Perl land there is also a lax
    version called UTF8 (without the hyphen). The Perl module <em>Encode</em>
    distinguishes these versions.
</p>

<p>
    Windows uses mostly UTF-16 which uses at least two bytes per codepoint,
    for very high codepoints it uses 4 bytes. There are two variants of
    UTF-16, which are marked with the suffix <code>LE</code> for "little
    endian" and <code>-BE</code> for "big endian" (see <a
    href="http://en.wikipedia.org/wiki/Endianness">Endianess</a>).
 </p>

<p>
    UTF-32 encodes every codepoint in 4 bytes. It is the only fixed width
    encoding that can implement the whole Unicode range.
</p>

    <table summary="Examples for character encodings">
    <tr>
        <th>Codepoint</th><th>Char</th><th>ASCII</th><th>UTF-8</th><th>Latin-1</th><th>ISO-8859-15</th><th>UTF-16</th>
    </tr>
    <tr>
        <td>U+0041</td><td>A</td><td>0x41</td><td>0x41</td><td>0x41</td><td>0x41</td><td>0x00 0x41</td> 
    </tr>
    <tr>
        <td>U+00c4    </td><td> Ä       </td><td>  -    </td><td> 0xc3 0x84
        </td><td> 0xc4    </td><td> 0xc4        </td><td> 0x00 0xc4</td>
    </tr>
    <tr>
        <td>U+20AC    </td><td> €       </td><td>  -    </td><td> 0xe3 0x82 0xac
        </td><td> -       </td><td> 0xa4        </td><td> 0x20 0xac</td>
    </tr>
    <tr>
        <td>U+c218    </td><td> 수      </td><td> -     </td><td> 0xec 0x88 0x98
        </td><td> -       </td><td> -           </td><td> 0xc2 0x18</td>
    </tr>
    </table>

<p>
    (The letter in the last line is the Hangul syllable SU, and your browser
    will only display it correctly if you have the appropriate Asian fonts
    installed.)
</p>

<p>
<a href="/images/unicode.png"><img src="/images/tn-unicode.png" width="380" height="400"
alt="Character encodings map from a character repertoire to byte sequences"
/></a>
<br />
Unicode defines a character repertoire of codepoints and their properties.
Character encodings like UTF-8 and UTF-16 define a way to write them as a
short sequence of bytes.
</p>

<h2><a name="perl_and_character_encodings"></a>Perl 5 and Character Encodings</h2>
<p>
    Perl Strings can either be used to hold text strings or binary data. Given
    a string, you generally have no mechanism of finding out whether it holds
    text or binary data - you have to keep track of it yourself.
</p>

<p>
    Interaction with the environment (like reading data from
    <code>STDIN</code> or a file, or printing it) treats strings as binary
    data.
    The same holds true for the return value of many built-in functions (like
    <code>gethostbyname</code>) and special variables that
    carry information to your program (<code>%ENV</code> and <code>@ARGV</code>).
</p>

<p>
    Other builtin functions that deal with text (like <code>uc</code> 
    and <code>lc</code> and regular expressions) treat strings as text, 
    or more accurately as a list of
    Codepoints.
</p>

<p>
    With the function <code>decode</code> in the module <a
    href="http://search.cpan.org/perldoc?Encode">Encode</a> you decode binary
    strings to make sure that the text handling functions work correctly.
</p>

<p>
    All text operations should work on strings that have been decoded by
    <code>Encode::decode</code> (or in other ways described below).

    Otherwise the text processing functions assume that the string is stored
    as Latin-1, which will yield incorrect results for any other encoding.
</p>

<p>
    Note that <code>cmp</code> only compares non-ASCII chars by codepoint
    number, which might give unexpected results. In general the ordering
    is language dependent, so that you need <code>use locale</code> in effect to
    sort strings according the rules of a natural language. For example, in
    German the desired ordering is <code>'a' lt 'ä' and 'ä' lt 'b'</code>,
    whereas comparison by codepoint number gives <code>'ä' gt 'b'</code>.
</p>

<pre>[%syntax perl%]
#!/usr/bin/perl
use warnings;
use strict;
use Encode qw(encode decode);

my $enc = 'utf-8'; # This script is stored as UTF-8
my $str = "Ä\n";

# Byte strings:
print lc $str; # prints 'Ä', lc didn't have any effect

# text strings::
my $text_str = decode($enc, $byte_str);
$text_str = lc $text_str;
print encode($enc, $text_str); # prints 'ä', lc worked as expected
[%endsyntax%]</pre>

<p>
    It is highly recommended to convert all input to text strings, then work
    with the text strings, and only covert them back to byte strings on output
    or storing.
</p>

<p>
    Otherwise, you can get confused very fast, and lose track of which strings
    are byte strings, and which ones are text strings.
</p>

<p>
    Perl offers IO layers, which are easy mechanisms to make these conversions
    automatically, either globally or per file handle.
</p>

    <pre>[%syntax perl%]
# IO layer: $handle now decodes all strings upon reading
open my $handle, '<:encoding(UTF-8)', $file;

# same
open my $handle, '<', $datei;
binmode $handle, ':encoding(UTF-8)';

# each open() automatically uses :encoding(iso-8859-1)
use open ':encoding(iso-8859-1)';

# All string literals in the script are interpreted as text strings:
use utf8;
# (assumes the script to be stored in UTF-8

# Get the current locale from the environment, and let STDOUT
# convert to that encoding:
use PerlIO::locale;
binmode STDOUT, ':locale';

# all I/O with current locale:
use open ':locale';
[%endsyntax%]</pre>
[% comment make vim happy again: ' %]

<p>
    Care should be taken with the input layer <code>:utf8</code>, which often
    pops up in example code and old documentation: it assumes
    the input to be in valid UTF-8, and you have no way of knowing in your
    program if that was actually the case.
    If not, it's a source of
    subtle security holes, see 
    <a href="http://www.perlmonks.org/?node_id=644786">this article on
    perlmonks.org</a> for details. Don't ever use it as an input layer,
    use <code>:encoding(UTF-8)</code> instead.
</p>

<p>
    The module and pragma <code>utf8</code> also allows you to use non-ASCII
    chars in variable names and module names. But beware, don't do this for
    package and module names; it might not work well.
    Also, consider that not everybody has a keyboard that allows easy typing of
    non-ASCII characters, so you make maintenance of your code much harder if
    you use them in your code.
</p>

    <h2><a name="testing_your_environment"></a>Testing your Environment</h2>
<p>
    You can use the following short script to your terminal, locales and
    fonts. It is very European centric, but you should be able to modify it to
    use the character encodings that are normally used where you live.
</p>

    <pre>[%syntax perl%]
#!/usr/bin/perl
use warnings;
use strict;
use Encode;

my @charsets = qw(utf-8 latin1 iso-8859-15 utf-16);

# some non-ASCII codepoints:
my $test = 'Ue: ' . chr(220) .'; Euro: '. chr(8364) . "\n";

for (@charsets){
    print "$_: " . encode($_, $test);
}
[%endsyntax%]</pre>
<p>
    If you run this program in a terminal, only one line will be displayed
    correctly, and its first column is the character encoding of your terminal.
</p>

<p>
    The Euro sign <code>€</code> isn't in Latin-1, so if your terminal has that
    encoding, the Euro sign won't be displayed correctly.
</p>

<p>
    Windows terminals mostly use <code>cp*</code> encodings, for example
    <code>cp850</code> or <code>cp858</code> (only available in new versions
    of Encode) for German windows installations. The rest of the operating
    environment uses <code>Windows-*</code> encodings, for example
    <code>Windows-1252</code> for a number of Western European localizations.
    <code>Encode->encodings(":all");</code> returns a list of
    all available encodings.
</p>


<h2><a name="troubleshooting"></a>Troubleshooting</h2>

<h3 id="wide-character" name="wide-character">"Wide Character in print"</h3>

<p>
    Sometimes you might see the <code>Wide character in print</code> warning.
</p>

<p>
    This means that you tried to use decoded string data in a context where it
    only makes sense to have binary data, in this case printing it.  You can
    make the warning go away by using an appropriate output layer, or by
    piping the offending string through <code>Encode::encode</code> first.
</p>

<h3 id="inspecting-strings" name="inspecting-strings">Inspecting Strings</h3>

<p>
    Sometimes you want to inspect if a string from an unknown source has
    already been decoded. Since Perl has no separate data types for binary
    strings and decoded strings, you can't do that reliably.
</p>

<p>
    But there is a way to guess the answer by using the module 
    <a href="http://search.cpan.org/perldoc?Devel::Peek">Devel::Peek</a></p>
    <pre>[%syntax perl%]
use Devel::Peek;
use Encode;
my $str = "ä";
Dump $str;
$str = decode("utf-8", $str);
Dump $str;
Dump encode('latin1', $str);[%endsyntax%]
__END__
SV = PV(0x814fb00) at 0x814f678
REFCNT = 1
FLAGS = (PADBUSY,PADMY,POK,pPOK)
PV = 0x81654f8 "\303\244"\0
CUR = 2
LEN = 4

SV = PV(0x814fb00) at 0x814f678
REFCNT = 1
FLAGS = (PADBUSY,PADMY,POK,pPOK,UTF8)
PV = 0x817fcf8 "\303\244"\0 [UTF8 "\x{e4}"]
CUR = 2
LEN = 4

SV = PV(0x814fb00) at 0x81b7f94
REFCNT = 1
FLAGS = (TEMP,POK,pPOK)
PV = 0x8203868 "\344"\0
CUR = 1
LEN = 4</pre>
<p>
    The string <code>UTF8</code> in the line starting with <code>FLAGS
    =</code> shows that the string has been decoded already.
    The line starting with <code>PV =</code> holds the bytes, and in brackets
    the codepoints.
</p>

<p>
    But there is a big caveat: Just because the <code>UTF8</code> flag isn't present
    doesn't mean that the text string hasn't been decoded. Perl uses either
    Latin-1 or UTF-8 internally to store strings, and the presence of this
    flag indicates which one is used.
</p>

<p>
    That also implies that if your program is written in Perl only (and has no
    XS components) it is <a
    href="fhttp://blogs.perl.org/users/aristotle/2011/08/utf8-flag.html">almost
    certainly an error to rely on the presence or
    absence of that flag.</a> You shouldn't care how perl stores its strings
    anyway.
</p>

<h3 id="buggy-modules" name="buggy-modules">Buggy Modules</h3>

<p>
    A common source of errors are buggy modules. The pragma
    <code>encoding</code> looks very tempting:
</p>

<pre>[%syntax perl%]
# automatic conversion to and from the current locale
use encoding ':locale';[%endsyntax%]</pre>

<p>
    But under the effect of <code>use encoding</code>, some AUTOLOAD
    functions stop working, and the module isn't thread safe.
</p>

<h2 name="charsets_in_the_www" id="charsets_in_the_www">Character Encodings in the WWW</h2>
<p>
    When you write a CGI script you have to chose a character encoding,
    print all your data in that encoding, and write it in the HTTP headers.
</p>
<p>
    For most applications, UTF-8 is a good choice, since you can code arbitrary
    Unicode codepoints with it. On the other hand English text (and of
    most other European languages) is encoded very efficiently.
</p>

<p>
    HTTP offers the <code>Accept-Charset</code>-Header in which the client can
    tell the server which character encodings it can handle. But if you stick
    to the common encodings like UTF-8 or Latin-1, next to all user agents
    will understand it, so it isn't really necessary to check that header.
</p>

<p>
    HTTP headers themselves are strictly ASCII only, so all information that
    is sent in the HTTP header (including cookies and URLs) need to be encoded
    to ASCII if non-ASCII characters are used.
</p>

<p>
    For HTML files the header typically looks like this:
    <code>Content-Type: text/html; charset=UTF-8</code>. If you send such a
    header, you only have to escape those characters that have a special
    meaninig in HTML: <code>&lt;</code>, <code>&gt;</code>, <code>&amp;</code>
    and, in attributes, <code>"</code>.
</p>
    
<p>
    Special care must be taken when reading POST or GET parameters with the
    function <code>param</code> in the module <code>CGI</code>. Older
    versions (prior to 3.29) always returned byte strings, newer version
    return text strings if <code>charset("UTF-8")</code> has been called
    before, and byte strings otherwise.
</p>

<p>
    CGI.pm also doesn't support character encodings other than UTF-8.
    Therefore you should not to use the <code>charset</code> routine
    and explicitly decode the parameter strings yourself.
</p>

<p>
    To ensure that form contents in the browser are sent with a known charset,
    you can add the <code>accept-charset</code> attribute to the
    <code>&lt;form&gt;</code> tag.
</p>
    <pre>[%syntax html%]
<form method="post" accept-charset="utf-8" action="/script.pl">[%endsyntax%]</pre>

    <!-- TODO: fix link-->
<p>
    If you use a template system, you should take care to choose one that
    knows how to handle character encodings. Good examples are 
    <a href="http://search.cpan.org/perldoc?Template::Alloy">Template::Alloy</a>,
    <a href="http://search.cpan.org/perldoc?HTML::Template::Compiled">HTML::Template::Compiled</a> 
    (since version 0.90 with the <code>open_mode</code> option), or
    Template Toolkit (with the <code>ENCODING</code> option in the constructor
    and an IO layer in the <code>process</code> method).</p>

    <h2 name="advanced_topics" id="advanced_topics">Advanced Topics</h2>
<p>
    With the basic charset and Perl knowledge you can get quite far. For
    example, you can make a web application "Unicode safe", i.e. you can
    take care
    that all possible user inputs are displayed correctly, in any script the
    user happens to use.
</p>

<p>
    But that's not all there is to know on the topic. For example, the Unicode
    standard allows different ways to compose some characters, so you need to
    "normalize" them before you can compare two strings. You can read more
    about that in the <a
    href="http://unicode.org/faq/normalization.html">Unicode normalization FAQ</a>.
</p>

<p>
    To implement country specific behaviour in programs, you should take a
    look at the locales system. For example in Turkey <code>lc 'I'</code>, the
    lower case of the capital letter I is <code>ı, U+0131 LATIN SMALL LETTER
    DOTLESS I</code>, while the upper case of <code>i</code> is <code>İ,
    U+0130 LATIN CAPITAL LETTER I WITH DOT ABOVE</code>.
</p>

<!-- TODO: link-->
<p>
    A good place to start reading about locales is <code>perldoc
    perllocale</code>.
</p>

<h2 id="philosophy">Philosophy</h2>

<p>
    Many programmers who are confronted with encoding issues first react with
    <i>"But shouldn't it just work?"</i>. Yes, it should just work. But too
    many systems are broken by design regarding character sets and encodings.
</p>

<h3 id="broken-by-design" name="broken-by-design">Broken by Design</h3>

<p>
    "Broken by Design" most of the time means that a document format, and API
    or a protocol allows multiple encodings, without a normative way on how
    that encoding information is transported and stored out of band.
</p>

<p>
    A classical example is the <a
    href="http://en.wikipedia.org/wiki/Internet_Relay_Chat">Internet Relay
    Chat (IRC)</a>, which specifies that a character is one Byte, but not
    which character encoding is used. This worked well in the Latin-1 days,
    but was bound to fail as soon as people from different continents started
    to use it.
</p>

<p>
    Currently, many IRC clients try to autodetect character encodings, and
    recode it to what the user configured. This works quite well in some
    cases, but produces really ugly results where it doesn't work.
</p>

<h3 id="broekn-xml" name="broken-xml">Another Example: XML</h3>

<p>
    The <a href="http://en.wikipedia.org/wiki/XML">Extensible Markup
    Language</a>, commonly known by its abbreviation XML, lets you specific
    the character encoding inside the file:
</p>

<pre>[% syntax xml %]
<?xml version="1.0" encoding="UTF-8" ?>
[% endsyntax %]</pre>

<p>
    There are two reasons why this is insufficient:
</p>

<ol>
    <li>The encoding information is optional. The specification clearly states
    that the encoding must be UTF-8 if the encoding information is absent, but
    sadly many tool authors don't seem to know that, end emit Latin-1. (This
    is of course only partly the fault of the specification).</li>
    <li>Any XML parser first has to autodetect the encoding to be able to
    parse the encoding information</li>
</ol>

<p>The second point is really important. You'd guess "Ah, that's no problem,
the preamble is just ASCII" - but many encodings are ASCII-incompatible in the
first 127 bytes (for example UTF-7, UCS-2 and UTF-16).</p>

<p>So although the encoding information is available, the parser first has to
guess nearly correctly to extract it.</p>

<p>The appendix to the XML specification contains a <a
href="http://www.w3.org/TR/2004/REC-xml-20040204/#sec-guessing-no-ext-info">detection
algorithm</a> than can handle all common cases, but for example lacks UTF-7
support.</p>

<h3 id="outofband">How to Do it Right: Out-of-band Signaling</h3>

<p>The XML example above demonstrates that a file format can't carry encoding
information in the file itself, unless you specify a way to carry that encoding
information on the byte level, independently of the encoding of the rest of
the file.</p>

<p>A possible workaround could have been to specific that the first line of
any XML file has to be ASCII encoded, and the rest of the file is in the
encoding that is specified in that first line. But it's an ugly workaround: a
normal text editor would display the first line completely wrong if the file
is in an ASCII-incompatible encoding. Of course it's also incompatible with
current XML specification, and would require a new, incompatible
specification, which would in turn break all existing applications.</p>

<p>So how to do it right, then?</p>

<p>The answer is quite simple: Every system that works with text data has to
either store meta data separately, or store everything in a uniform
encoding.</p>

<p>It is tempting to store everything in the same encoding, and it works quite
well on a local machine, but you can't expect everyone to agree on one single
encoding, so all data exchange still has to carry encoding information. And
usually you want to store original files (for fear of data loss), so you have
to keep that encoding information somewhere.</p>

<p>This observation should have a huge impact on the computing world: all file
systems should allow you to store encoding information as meta data, and
easily retrieve that meta data. The same should hold true for file names, and
programming languages (at least those who want to take the pain away from
their users) should transparently transport that meta information, and take
care of all encoding issues.</p>

<p>Then it could just work.</p>

<h2 id="further_reading" name="further_reading">Further reading</h2>

<ul>
    <li><a
        href="http://www.w3.org/International/tutorials/tutorial-char-enc/">W3c
        tutorial on character encodings in HTML and CSS</a></li>
    <li><a
        href="http://en.wikibooks.org/wiki/Perl_Programming/Unicode_UTF-8">Perl
        Programming/Unicode UTF-8 wikibook</a></li>
    <li><a href="http://perldoc.perl.org/perlunitut.html">perlunitut, the Perl
        Unicode Tutorial</a></li>
</ul>

<h2 id="tools" name="tools">Useful Tools</h2>

<ul>
    <li>
        <a href="http://live.gnome.org/Gucharmap">gucharmap</a>, the Gnome
        Unicode character map.
    </li>
    <li>
        <a href="/blog-en/perl-tips/utf8-dump.html">An UTF-8 dumper</a> that
        shows you the name of non-ASCII characters.
    </li>
    <li>
        <a href="http://www.freebsd.org/cgi/man.cgi?query=hexdump">hexdump
        never lies</a> (on Debian it's in the <code>bsdmainutils</code>
        package).
    </li>
    <li>
        <a href="http://www.gnu.org/software/libiconv/documentation/libiconv/iconv.1.html">iconv</a>
        converts text files from one character encoding to another.
    </li>
</ul>


<h2 id="acknowledgements" name="acknowledgements">Acknowledgments</h2>


<p>
    This article is a translation of a <a
    href="/de/artikel/charsets-unicode">German article of mine</a> written
    for <a href="http://foo-magazin.de/">$foo-Magazin 01/2008</a>, a German
    Perl magazine. It was enhanced and corrected since then.
</p>

<p>
    Many thanks go to <a href="http://juerd.nl">Juerd Waalboer</a>, who
    pointed out many smaller and a few
    not-so-small errors in earlier versions of this article, and contributed
    greatly to my understanding of Perl's string handling.
</p>

<p>
    I'd also like to thank <a
    href="http://www.perlmonks.org/?node=ELISHEVA">ELISHEVA</a>
    for suggesting many improvements to both grammar and spelling.
</p>

<p>I'd like to acknowledge insightful discussions with:</p>
<ul>
    <li><a href="http://www.perlmonks.org/?node=ikegami">ikegami</a></li>
    <li><a href="http://plasmasturm.org/">Aristotle Pagaltzis</a></li>
</ul>

[% comment vim: spell ft=html
%]
